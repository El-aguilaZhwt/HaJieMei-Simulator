#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦é£æ ¼è¯„è®ºå›å¤ç”Ÿæˆå™¨ï¼ˆè¿ç»­å¯¹è¯ç‰ˆï¼‰
åŸºäºLoRAå¾®è°ƒçš„Qwen3-14Bæ¨¡å‹
"""

from unsloth import FastLanguageModel
from transformers import AutoTokenizer
import torch
import readline
import re

def load_model():
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name="/home/xiuyu/models/Qwen3-14B-unsloth-bnb-4bit",
            max_seq_length=2048,
            dtype=torch.float16,
            load_in_4bit=True,
            device_map="auto",
        )
        
        # æ·»åŠ ç‰¹æ®Štoken
        special_tokens = ["<|im_start|>", "<|im_end|>"]
        tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
        model.resize_token_embeddings(len(tokenizer))
        
        # åŠ è½½LoRAé€‚é…å™¨
        model.load_adapter("/home/xiuyu/qwen3train/xiaohongshu_train/checkpoint-6492")
        
        model.config.use_cache = True
        model.config.pretraining_tp = 1
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        return model, tokenizer
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise

def generate_reply(model, tokenizer, conversation_history, max_length=150):
    try:
        # æ„å»ºå®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡
        system_prompt = "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„è¯„è®ºå›å¤ç”Ÿæˆå™¨ï¼Œæ“…é•¿ç”¨åˆ»è–„ã€åˆé’»çš„è¯­æ°”å›å¤å„ç§è¯„è®º<|im_end|>\n"
        full_context = system_prompt + "\n".join(conversation_history)
        
        # ç¡®ä¿ä¸Šä¸‹æ–‡ä¸è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦
        inputs = tokenizer(full_context, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.85,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
        )
        
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        reply = full_text.split("<|im_start|>assistant\n")[-1]
        reply = reply.split("<|im_end|>")[0].strip()
        
        # åå¤„ç†ï¼šç¡®ä¿å›å¤åŒ…å«æ–‡å­—å†…å®¹
        reply = ensure_text_content(reply)
        return reply
        
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        return "âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ç¼©çŸ­è¾“å…¥"
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

def ensure_text_content(text):
    """ç¡®ä¿å›å¤åŒ…å«å®è´¨æ€§æ–‡å­—å†…å®¹"""
    # å¦‚æœåªæœ‰è¡¨æƒ…ç¬¦å·
    if re.fullmatch(r'($$[^]]+$$\s*)+', text):
        additions = [
            "è¿™æ³¢æ“ä½œæˆ‘ç»™æ»¡åˆ†ï¼",
            "å§å¦¹è¯´å¾—å¤ªå¯¹äº†ï¼",
            "æˆ‘çœŸçš„ä¼šè°¢ï¼",
            "è¿™ç®€ç›´æ˜¯æˆ‘çš„äº’è”ç½‘å˜´æ›¿ï¼",
            "ç¬‘ä¸æ´»äº†å®¶äººä»¬ï¼"
        ]
        return f"{text} {additions[len(text) % len(additions)]}"
    return text

def main():
    try:
        model, tokenizer = load_model()
        
        print("\nğŸ€ å°çº¢ä¹¦é£æ ¼è¯„è®ºå›å¤ç”Ÿæˆå™¨ï¼ˆè¿ç»­å¯¹è¯ç‰ˆï¼‰ ğŸ€")
        print("è¾“å…¥åˆå§‹è¯„è®ºå’Œè¿­ä»£æ¬¡æ•°ï¼ˆä¾‹å¦‚ï¼š'ä½ å¥½ 3' è¡¨ç¤ºç”Ÿæˆ3å±‚å›å¤ï¼‰")
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                print("\n>>> åˆå§‹è¯„è®º+è¿­ä»£æ¬¡æ•° (ç”¨ç©ºæ ¼åˆ†éš”):")
                user_input = input().strip()
                
                if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
                    print("å†è§ï¼")
                    break
                    
                if not user_input:
                    print("è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                    continue
                
                # è§£æè¾“å…¥
                parts = user_input.split()
                if len(parts) < 2:
                    initial_comment = user_input
                    iterations = 1
                else:
                    try:
                        initial_comment = " ".join(parts[:-1])
                        iterations = min(int(parts[-1]), 35)  # é™åˆ¶æœ€å¤š5æ¬¡è¿­ä»£
                    except:
                        initial_comment = user_input
                        iterations = 1
                
                # åˆå§‹åŒ–å¯¹è¯å†å²
                conversation_history = [
                    f"<|im_start|>user\n{initial_comment}<|im_end|>"
                ]
                
                print(f"\nğŸ’¬ åˆå§‹è¯„è®º: {initial_comment}")
                print(f"ğŸ”„ å°†ç”Ÿæˆ {iterations} å±‚å›å¤...\n")
                
                # ç”Ÿæˆè¿ç»­å›å¤
                for i in range(iterations):
                    print(f"\nğŸ”„ æ­£åœ¨ç”Ÿæˆç¬¬ {i+1} å±‚å›å¤...")
                    reply = generate_reply(model, tokenizer, conversation_history)
                    
                    # æ·»åŠ åˆ°å¯¹è¯å†å²
                    conversation_history.append(
                        f"<|im_start|>assistant\n{reply}<|im_end|>\n"
                        f"<|im_start|>user\nç»§ç»­è¿™ä¸ªè¯é¢˜<|im_end|>"
                    )
                    
                    # æ‰“å°å›å¤
                    print(f"\nğŸ“Œ ç¬¬ {i+1} å±‚å›å¤:")
                    print("-" * 50)
                    print(reply)
                    print("-" * 50)
                
            except KeyboardInterrupt:
                print("\nä½¿ç”¨Ctrl+Cé€€å‡º")
                break
                
    finally:
        if 'model' in locals():
            del model
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()