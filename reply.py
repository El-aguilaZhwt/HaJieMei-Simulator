#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦é£æ ¼è¯„è®ºå›å¤ç”Ÿæˆå™¨
åŸºäºLoRAå¾®è°ƒçš„Qwen3-14Bæ¨¡å‹
"""

from unsloth import FastLanguageModel
from transformers import AutoTokenizer
import torch
import readline  # ç”¨äºæ”¹è¿›è¾“å…¥ä½“éªŒ

# åˆå§‹åŒ–æ¨¡å‹
def load_model():
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    try:
        # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name="/home/xiuyu/models/Qwen3-14B-unsloth-bnb-4bit",  # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„åŸå§‹æ¨¡å‹
            max_seq_length=2048,
            dtype=torch.float16,
            load_in_4bit=True,
            device_map="auto",
        )
        
        # æ·»åŠ ç‰¹æ®Štoken
        special_tokens = ["<|im_start|>", "<|im_end|>"]
        tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
        model.resize_token_embeddings(len(tokenizer))
        
        # ç„¶ååŠ è½½LoRAé€‚é…å™¨
        model.load_adapter("/home/xiuyu/qwen3train/xiaohongshu_train/checkpoint-6492")
        
        # æ˜¾å­˜ä¼˜åŒ–é…ç½®
        model.config.use_cache = True
        model.config.pretraining_tp = 1 
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        return model, tokenizer
        
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise

# ç”Ÿæˆå›å¤
def generate_reply(model, tokenizer, prompt, max_length=150):
    try:
        # æ„å»ºå¯¹è¯æ ¼å¼
        text = (
            f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„è¯„è®ºå›å¤ç”Ÿæˆå™¨ï¼Œ"
            f"æ“…é•¿ç”¨åˆ»è–„ã€åˆé’»çš„è¯­æ°”å›å¤å„ç§è¯„è®º<|im_end|>\n"
            f"<|im_start|>user\n{prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        
        inputs = tokenizer(text, return_tensors="pt").to("cuda")
        
        # ç”Ÿæˆé…ç½®
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.85,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
        )
        
        # æå–ç”Ÿæˆçš„å›å¤éƒ¨åˆ†
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
        reply = full_text.split("<|im_start|>assistant\n")[-1]
        reply = reply.split("<|im_end|>")[0].strip()
        
        return reply
        
    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        return "âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ç¼©çŸ­è¾“å…¥æˆ–å‡å°‘max_length"
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

# ä¸»äº¤äº’å¾ªç¯
def main():
    try:
        model, tokenizer = load_model()
        
        print("\nğŸ€ å°çº¢ä¹¦é£æ ¼è¯„è®ºå›å¤ç”Ÿæˆå™¨ ğŸ€")
        print("è¾“å…¥ä½ çš„è¯„è®º(è¾“å…¥'é€€å‡º'ç»“æŸ):")
        print(f"å½“å‰è®¾å¤‡: {model.device} | æœ€å¤§é•¿åº¦: 2048 tokens")
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥ï¼ˆæ”¯æŒå¤šè¡Œï¼Œç”¨Ctrl+Dç»“æŸï¼‰
                print("\n>>> ä½ çš„è¯„è®º: (Ctrl+Dç»“æŸè¾“å…¥)")
                user_input = []
                while True:
                    try:
                        line = input()
                        user_input.append(line)
                    except EOFError:
                        break
                user_input = "\n".join(user_input).strip()
                
                if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
                    print("å†è§ï¼")
                    break
                    
                if not user_input:
                    print("è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")
                    continue
                    
                # ç”Ÿæˆå›å¤
                print("\nğŸ”„ ç”Ÿæˆå›å¤ä¸­...")
                reply = generate_reply(model, tokenizer, user_input)
                
                # ç¾åŒ–è¾“å‡º
                print("\nğŸ’– å°çº¢ä¹¦é£æ ¼å›å¤:")
                print("-" * 50)
                print(reply)
                print("-" * 50)
                
            except KeyboardInterrupt:
                print("\nä½¿ç”¨Ctrl+Cé€€å‡º")
                break
                
    finally:
        # æ¸…ç†æ˜¾å­˜
        if 'model' in locals():
            del model
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()